<!DOCTYPE html>
<html lang="uz">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="stele.css"> <!-- Sizning maxsus uslublaringiz -->
    <title>Ehtimollar Nazariyasi va Matematik Statistika (1-10)</title>
</head>
<body>

    <div class="accordion-container">
        

    <!-- 1-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">1. Distribution function of a random vector and its properties. Random variables of discrete and continuous type.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>The concepts related to the distribution of random variables, both single and multiple (vectors), are fundamental in probability theory.</p>

                <h5>Distribution Function of a Random Vector (Joint CDF)</h5>
                <p>For a random vector (X₁, X₂, ..., Xₙ), the joint cumulative distribution function (CDF) is defined as:</p>
                <pre class="formula">F(x₁, x₂, ..., xₙ) = P(X₁ ≤ x₁, X₂ ≤ x₂, ..., Xₙ ≤ xₙ)</pre>
                <p>This function gives the probability that each component Xᵢ of the random vector is less than or equal to the corresponding value xᵢ.</p>

                <h5>Properties of the Joint CDF:</h5>
                <ul>
                    <li><strong>Non-negativity and Boundedness:</strong> 0 ≤ F(x₁, ..., xₙ) ≤ 1.</li>
                    <li><strong>Non-decreasing:</strong> F is non-decreasing in each argument. If xᵢ ≤ x'ᵢ, then F(..., xᵢ, ...) ≤ F(..., x'ᵢ, ...).</li>
                    <li><strong>Limits:</strong>
                        <ul>
                            <li>lim (xᵢ→-∞) F(x₁, ..., xₙ) = 0 for any i. (If any component goes to -∞, the probability is 0).</li>
                            <li>lim (all xᵢ→+∞) F(x₁, ..., xₙ) = 1. (If all components go to +∞, the probability is 1).</li>
                        </ul>
                    </li>
                    <li><strong>Right-continuity:</strong> F is right-continuous in each argument.</li>
                </ul>

                <h5>Random Variables of Discrete Type:</h5>
                <p>A random variable X is discrete if it can take on a finite or countably infinite number of distinct values {x₁, x₂, ...}. Its distribution is typically described by a <strong>Probability Mass Function (PMF)</strong>, p(xᵢ) = P(X = xᵢ), such that:</p>
                <ul>
                    <li>p(xᵢ) ≥ 0 for all i.</li>
                    <li>Σᵢ p(xᵢ) = 1.</li>
                </ul>

                <h5>Random Variables of Continuous Type:</h5>
                <p>A random variable X is continuous if it can take on any value within a given range or interval (or multiple intervals). The probability that X takes any single specific value is zero, i.e., P(X=c) = 0. Its distribution is described by a <strong>Probability Density Function (PDF)</strong>, f(x), such that:</p>
                <ul>
                    <li>f(x) ≥ 0 for all x.</li>
                    <li>∫<sub>-∞</sub><sup>+∞</sup> f(x)dx = 1.</li>
                    <li>P(a ≤ X ≤ b) = ∫<sub>a</sub><sup>b</sup> f(x)dx.</li>
                </ul>
                <p>The CDF for a continuous random variable is F(x) = ∫<sub>-∞</sub><sup>x</sup> f(t)dt, and f(x) = F'(x) where the derivative exists.</p>
            </div>
        </div>
    </div>
    <!-- 1-MAVZU TUGASHI -->

    <!-- 2-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">2. Correlation coefficient and its properties.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>The correlation coefficient is a statistical measure that describes the strength and direction of a <strong>linear</strong> relationship between two random variables or two sets of data.</p>

                <h5>Definition:</h5>
                <p>The Pearson correlation coefficient (ρ for a population, r for a sample) between two random variables X and Y is calculated as:</p>
                <pre class="formula">ρ(X,Y) = Cov(X,Y) / (σₓ * σᵧ)</pre>
                <p>Where:</p>
                <ul>
                    <li><strong>Cov(X,Y)</strong> is the covariance between X and Y. Cov(X,Y) = E[(X - μₓ)(Y - μᵧ)].</li>
                    <li><strong>σₓ</strong> is the standard deviation of X.</li>
                    <li><strong>σᵧ</strong> is the standard deviation of Y.</li>
                    <li><strong>μₓ</strong> and <strong>μᵧ</strong> are the means of X and Y, respectively.</li>
                </ul>

                <h5>Properties of the Correlation Coefficient:</h5>
                <ul>
                    <li><strong>Range:</strong> The value of ρ (or r) always lies between -1 and +1, inclusive.
                        <pre class="formula">-1 ≤ ρ ≤ +1</pre>
                    </li>
                    <li><strong>Interpretation of Values:</strong>
                        <ul>
                            <li><strong>ρ = +1:</strong> Perfect positive linear relationship. As X increases, Y increases proportionally.</li>
                            <li><strong>ρ = -1:</strong> Perfect negative linear relationship. As X increases, Y decreases proportionally.</li>
                            <li><strong>ρ = 0:</strong> No linear relationship. This does not necessarily mean there is no relationship at all; there could be a non-linear relationship.</li>
                            <li>Values between 0 and +1 indicate varying degrees of positive linear relationship.</li>
                            <li>Values between 0 and -1 indicate varying degrees of negative linear relationship.</li>
                        </ul>
                    </li>
                    <li><strong>Symmetry:</strong> The correlation between X and Y is the same as the correlation between Y and X.
                        <pre class="formula">ρ(X,Y) = ρ(Y,X)</pre>
                    </li>
                    <li><strong>Dimensionless:</strong> The correlation coefficient has no units.</li>
                    <li><strong>Invariance to Linear Transformations:</strong> The correlation coefficient is invariant to changes in scale and origin of the variables. If X' = aX + b and Y' = cY + d (where a and c are non-zero constants):
                        <ul>
                            <li>ρ(X',Y') = ρ(X,Y) if a and c have the same sign.</li>
                            <li>ρ(X',Y') = -ρ(X,Y) if a and c have opposite signs.</li>
                        </ul>
                    </li>
                    <li><strong>Correlation and Independence:</strong> If X and Y are independent, then ρ(X,Y) = 0. However, the converse is not always true (ρ=0 does not necessarily imply independence, except for normally distributed variables).</li>
                </ul>
            </div>
        </div>
    </div>
    <!-- 2-MAVZU TUGASHI -->

    <!-- 3-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">3. Primary statistical analysis of a sample. Variation series.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>Primary statistical analysis of a sample involves the initial steps of organizing, summarizing, and describing the main features of a dataset (sample) obtained from a larger population.</p>

                <h5>Steps in Primary Statistical Analysis:</h5>
                <ol>
                    <li>
                        <strong>Data Collection and Organization:</strong>
                        <ul>
                            <li>Gathering raw data.</li>
                            <li>Checking for errors or inconsistencies.</li>
                            <li>Arranging the data, often by creating a variation series.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Variation Series (Ordered Series):</strong>
                        <p>A variation series is a sample of data arranged in non-decreasing (or non-increasing) order of magnitude. If a sample consists of n observations x₁, x₂, ..., xₙ, the variation series is denoted as x₍₁₎ ≤ x₍₂₎ ≤ ... ≤ x₍ₙ₎, where x₍ᵢ₎ is the i-th order statistic.</p>
                        <p><em>Example:</em> If sample data is {5, 2, 8, 2, 5}, the variation series is {2, 2, 5, 5, 8}.</p>
                    </li>
                    <li>
                        <strong>Frequency Distribution:</strong>
                        <ul>
                            <li>Grouping data into classes or intervals (for continuous or discrete data with many values).</li>
                            <li>Counting the frequency (number of occurrences) of each value or within each class.</li>
                            <li>Calculating relative frequencies (frequency / total number of observations).</li>
                            <li>Calculating cumulative frequencies.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Calculation of Summary Statistics:</strong>
                        <ul>
                            <li><strong>Measures of Central Tendency:</strong> Describe the "center" of the data (e.g., mean, median, mode).</li>
                            <li><strong>Measures of Dispersion (Variability):</strong> Describe the "spread" or "scatter" of the data (e.g., range, variance, standard deviation, interquartile range).</li>
                            <li><strong>Measures of Shape:</strong> Describe the asymmetry (skewness) and peakedness (kurtosis) of the distribution.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Graphical Representation:</strong>
                        <ul>
                            <li>Visualizing the data using graphs like histograms, frequency polygons, bar charts, pie charts, box plots, stem-and-leaf plots, etc.</li>
                        </ul>
                    </li>
                </ol>
                <p>The variation series is a fundamental first step as it helps in easily identifying the minimum, maximum, range, and facilitates the calculation of other statistics like the median and quartiles, as well as constructing frequency distributions.</p>
            </div>
        </div>
    </div>
    <!-- 3-MAVZU TUGASHI -->

    <!-- 4-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">4. Graphs of variation series. Empirical distribution function.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>Visualizing data from a variation series and understanding its cumulative distribution are key aspects of exploratory data analysis.</p>

                <h5>Graphs of Variation Series:</h5>
                <p>Once a variation series (ordered data) is available, and often a frequency distribution is constructed from it, several graphical representations can be used:</p>
                <ul>
                    <li>
                        <strong>Histogram:</strong>
                        <p>A bar chart where the horizontal axis represents the class intervals (or discrete values) and the vertical axis represents the frequency (or relative frequency). The area of each bar is proportional to the frequency of data points in that interval. Histograms show the shape of the data distribution.</p>
                    </li>
                    <li>
                        <strong>Frequency Polygon:</strong>
                        <p>Formed by joining the midpoints of the tops of the bars in a histogram with straight lines. It provides a smoother representation of the distribution shape. The polygon is usually "closed" by extending lines to the horizontal axis one class interval beyond the first and last classes.</p>
                    </li>
                    <li>
                        <strong>Ogive (Cumulative Frequency Polygon):</strong>
                        <p>A graph that shows the cumulative frequency. It plots the cumulative frequencies against the upper class boundaries (for "less than" ogive) or lower class boundaries (for "more than" ogive). The "less than" ogive is more common and visually represents the empirical distribution function.</p>
                    </li>
                    <li>
                        <strong>Box Plot (Box-and-Whisker Plot):</strong>
                        <p>A standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. It is useful for identifying outliers and comparing distributions.</p>
                    </li>
                </ul>

                <h5>Empirical Distribution Function (EDF), F̂(x):</h5>
                <p>The empirical distribution function (EDF), often denoted as F̂(x) or Fₙ(x), is an estimate of the true underlying cumulative distribution function (CDF) of the population from which the sample was drawn. It is a step function that jumps up by 1/n at each of the n data points in an ordered sample.</p>
                <p>For a sample x₁, x₂, ..., xₙ, the EDF is defined as:</p>
                <pre class="formula">F̂(x) = (Number of observations ≤ x) / n</pre>
                <p>Alternatively, if x₍₁₎ ≤ x₍₂₎ ≤ ... ≤ x₍ₙ₎ is the ordered sample (variation series):</p>
                <pre class="formula">
F̂(x) = 0,            if x < x₍₁₎
F̂(x) = k/n,         if x₍k₎ ≤ x < x₍k+1₎, for k = 1, ..., n-1
F̂(x) = 1,            if x ≥ x₍ₙ₎
                </pre>
                <p><strong>Properties of EDF:</strong></p>
                <ul>
                    <li>It is a step function.</li>
                    <li>It is non-decreasing.</li>
                    <li>0 ≤ F̂(x) ≤ 1.</li>
                    <li>As the sample size n increases, the EDF F̂(x) converges to the true CDF F(x) (Glivenko-Cantelli theorem).</li>
                </ul>
                <p>The "less than" ogive is a graphical representation of the empirical distribution function.</p>
            </div>
        </div>
    </div>
    <!-- 4-MAVZU TUGASHI -->

    <!-- 5-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">5. Numerical characteristics of a sample.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>Numerical characteristics of a sample, also known as sample statistics, are values calculated from sample data to summarize its key features. They provide quantitative descriptions of the data's central tendency, dispersion, and shape, serving as estimates for the corresponding population parameters.</p>

                <h5>Measures of Central Tendency (Location):</h5>
                <p>These statistics describe the "center" or typical value of the dataset.</p>
                <ul>
                    <li><strong>Sample Mean (x̄):</strong> The arithmetic average of the observations.
                        <pre class="formula">x̄ = (Σ xᵢ) / n</pre>
                    </li>
                    <li><strong>Sample Median (Me or x̃):</strong> The middle value when the data is ordered. If n is odd, it's the (n+1)/2-th value. If n is even, it's usually the average of the n/2-th and (n/2 + 1)-th values.</li>
                    <li><strong>Sample Mode (Mo):</strong> The value(s) that occur most frequently in the dataset. A dataset can be unimodal, bimodal, multimodal, or have no mode.</li>
                </ul>

                <h5>Measures of Dispersion (Variability or Spread):</h5>
                <p>These statistics describe how spread out or scattered the data points are.</p>
                <ul>
                    <li><strong>Sample Range (R):</strong> The difference between the maximum and minimum values in the sample.
                        <pre class="formula">R = x<sub>max</sub> - x<sub>min</sub></pre>
                    </li>
                    <li><strong>Sample Variance (s²):</strong> The average of the squared deviations of each observation from the sample mean.
                        <pre class="formula">s² = [Σ (xᵢ - x̄)²] / (n-1)</pre>
                        (Using n-1 in the denominator provides an unbiased estimator of the population variance.)
                    </li>
                    <li><strong>Sample Standard Deviation (s):</strong> The square root of the sample variance. It measures the typical deviation of observations from the mean, in the original units of the data.
                        <pre class="formula">s = √s²</pre>
                    </li>
                    <li><strong>Interquartile Range (IQR):</strong> The difference between the third quartile (Q3, 75th percentile) and the first quartile (Q1, 25th percentile). It measures the spread of the middle 50% of the data and is less sensitive to outliers than the range.
                        <pre class="formula">IQR = Q3 - Q1</pre>
                    </li>
                    <li><strong>Coefficient of Variation (CV):</strong> The ratio of the standard deviation to the mean, often expressed as a percentage. It's a relative measure of dispersion.
                        <pre class="formula">CV = (s / |x̄|) * 100%</pre>
                    </li>
                </ul>

                <h5>Measures of Shape:</h5>
                <p>These statistics describe the shape of the data distribution.</p>
                <ul>
                    <li><strong>Sample Skewness (g₁):</strong> Measures the asymmetry of the distribution.
                        <ul>
                            <li>g₁ > 0: Positively skewed (tail to the right).</li>
                            <li>g₁ < 0: Negatively skewed (tail to the left).</li>
                            <li>g₁ ≈ 0: Approximately symmetric.</li>
                        </ul>
                    </li>
                    <li><strong>Sample Kurtosis (g₂):</strong> Measures the "peakedness" or "tailedness" of the distribution relative to a normal distribution.
                        <ul>
                            <li>g₂ > 0 (Leptokurtic): More peaked, heavier tails.</li>
                            <li>g₂ < 0 (Platykurtic): Flatter, lighter tails.</li>
                            <li>g₂ ≈ 0 (Mesokurtic): Similar peakedness to a normal distribution. (Excess kurtosis is often reported, where normal distribution has excess kurtosis of 0).</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>
    </div>
    <!-- 5-MAVZU TUGASHI -->

    <!-- 6-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">6. Distribution as statistical estimates of non-normal parameters.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>This topic refers to the use of <strong>sampling distributions</strong> of estimators to make inferences (like constructing confidence intervals or performing hypothesis tests) about population parameters, especially when these parameters belong to non-normal distributions, or when the parameters themselves are not means (e.g., variances, proportions).</p>

                <h5>Key Concepts:</h5>
                <ul>
                    <li>
                        <strong>Estimators and Sampling Distributions:</strong>
                        <p>An estimator (e.g., sample mean x̄, sample variance s², sample proportion p̂) is a statistic calculated from a sample to estimate an unknown population parameter (e.g., population mean μ, population variance σ², population proportion p). Since estimators vary from sample to sample, they are random variables and have their own probability distributions, known as <strong>sampling distributions</strong>.</p>
                    </li>
                    <li>
                        <strong>Central Limit Theorem (CLT):</strong>
                        <p>Even if the underlying population distribution is not normal, the sampling distribution of the sample mean (x̄) tends to be approximately normal for sufficiently large sample sizes (usually n ≥ 30). This is crucial for making inferences about the population mean μ.</p>
                    </li>
                    <li>
                        <strong>Estimating Parameters of Non-Normal Distributions:</strong>
                        <p>Many real-world phenomena are not normally distributed (e.g., exponential, Poisson, binomial, gamma distributions). We still need to estimate their parameters.</p>
                        <ul>
                            <li><strong>Example (Binomial):</strong> If we want to estimate the success probability 'p' of a binomial distribution, we use the sample proportion p̂ = k/n (number of successes / number of trials). For large n, the sampling distribution of p̂ can be approximated by a normal distribution.</li>
                            <li><strong>Example (Poisson):</strong> The parameter λ (mean rate) of a Poisson distribution can be estimated by the sample mean x̄. For large λ, the Poisson distribution itself can be approximated by a normal distribution.</li>
                            <li><strong>Example (Exponential):</strong> The rate parameter λ of an exponential distribution is often estimated by 1/x̄. The sampling distribution of x̄ (and thus 1/x̄) can be complex but may involve distributions like the Gamma distribution.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Estimating Non-Mean Parameters (e.g., Variance):</strong>
                        <p>If the population is normal, the sampling distribution of the sample variance s² (when scaled) follows a <strong>Chi-squared (χ²) distribution</strong>. This allows us to make inferences about the population variance σ².</p>
                        <pre class="formula">(n-1)s² / σ² ~ χ²<sub>(n-1)</sub></pre>
                    </li>
                    <li>
                        <strong>Use of t-distribution:</strong>
                        <p>When estimating the mean μ of a normally distributed population with an unknown population standard deviation σ, and using the sample standard deviation s, the statistic (x̄ - μ) / (s/√n) follows a <strong>t-distribution</strong> with n-1 degrees of freedom. This is also robust to moderate departures from normality, especially for larger sample sizes.</p>
                    </li>
                    <li>
                        <strong>Bootstrap Methods:</strong>
                        <p>For complex distributions or when analytical sampling distributions are unknown, resampling methods like bootstrapping can be used to approximate the sampling distribution of an estimator and construct confidence intervals, even for non-normal parameters.</p>
                    </li>
                </ul>
                <p>In summary, statistical inference often relies on knowing or approximating the sampling distribution of an estimator. While normality assumptions simplify many procedures, various techniques and distributions (CLT, Chi-squared, t-distribution, approximations for proportions, bootstrapping) allow us to estimate and make inferences about parameters from non-normal distributions or parameters other than the mean.</p>
            </div>
        </div>
    </div>
    <!-- 6-MAVZU TUGASHI -->

    <!-- 7-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">7. Method of least squares (MLS). Mean approximation error. Average coefficient of elasticity. Coefficient of determination.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>These concepts are central to regression analysis, which aims to model the relationship between a dependent variable and one or more independent variables.</p>

                <h5>Method of Least Squares (MLS):</h5>
                <p>The Method of Least Squares is a standard approach in regression analysis used to find the "best fit" line or curve for a set of data points. It works by minimizing the sum of the squares of the residuals (the differences between the observed values of the dependent variable and the values predicted by the model).</p>
                <p>For a simple linear regression model ŷ = b₀ + b₁x:</p>
                <ul>
                    <li>yᵢ is the observed value for the i-th observation.</li>
                    <li>ŷᵢ is the predicted value for the i-th observation.</li>
                    <li>The residual for the i-th observation is eᵢ = yᵢ - ŷᵢ.</li>
                </ul>
                <p>MLS finds the values of the coefficients (b₀ and b₁) that minimize the Sum of Squared Residuals (SSR) or Sum of Squared Errors (SSE):</p>
                <pre class="formula">SSR = Σ eᵢ² = Σ (yᵢ - ŷᵢ)² = Σ (yᵢ - (b₀ + b₁xᵢ))²</pre>

                <h5>Mean Approximation Error (often Mean Squared Error - MSE):</h5>
                <p>The Mean Squared Error (MSE) is a common measure of the average squared difference between the estimated values and the actual values. It quantifies the quality of an estimator or a model's predictions.</p>
                <p>In the context of regression, MSE is calculated as:</p>
                <pre class="formula">MSE = SSR / (n - k - 1)   or sometimes   MSE = SSR / n</pre>
                <p>Where:</p>
                <ul>
                    <li>SSR is the Sum of Squared Residuals.</li>
                    <li>n is the number of observations.</li>
                    <li>k is the number of independent variables in the model. (n - k - 1) represents the degrees of freedom for the error.</li>
                </ul>
                <p>The <strong>Root Mean Squared Error (RMSE)</strong> = √MSE, is often used as it is in the same units as the dependent variable.</p>

                <h5>Average Coefficient of Elasticity:</h5>
                <p>In economics, elasticity measures the responsiveness of one variable to a percentage change in another. For a function y = f(x), point elasticity is defined as:</p>
                <pre class="formula">Elasticity (E) = (% change in y) / (% change in x) = (dy/y) / (dx/x) = (dy/dx) * (x/y)</pre>
                <p>The "average coefficient of elasticity" in a regression context often refers to calculating this elasticity at the mean values of the variables (x̄, ȳ) using the estimated regression coefficients.</p>
                <p>For a linear model y = b₀ + b₁x, the slope b₁ = dy/dx. So, average elasticity is:</p>
                <pre class="formula">E_avg = b₁ * (x̄ / ȳ)</pre>
                <p>For a log-log model (ln(y) = β₀ + β₁ln(x)), the coefficient β₁ itself is the elasticity.</p>

                <h5>Coefficient of Determination (R²):</h5>
                <p>The Coefficient of Determination (R-squared) is a statistic that indicates the proportion of the variance in the dependent variable (y) that is predictable from the independent variable(s) (x) in a regression model. It ranges from 0 to 1 (or 0% to 100%).</p>
                <ul>
                    <li>An R² of 1 indicates that the regression predictions perfectly fit the data.</li>
                    <li>An R² of 0 indicates that the model explains none of the variability of the response data around its mean.</li>
                </ul>
                <p>R² is calculated as:</p>
                <pre class="formula">R² = 1 - (SSR / SST) = (SST - SSR) / SST = SSM / SST</pre>
                <p>Where:</p>
                <ul>
                    <li><strong>SSR (Sum of Squared Residuals):</strong> Σ (yᵢ - ŷᵢ)² (unexplained variation)</li>
                    <li><strong>SST (Total Sum of Squares):</strong> Σ (yᵢ - ȳ)² (total variation in y)</li>
                    <li><strong>SSM (Sum of Squares Model or Regression Sum of Squares):</strong> Σ (ŷᵢ - ȳ)² (explained variation)</li>
                </ul>
                <p>A higher R² generally indicates a better fit of the model to the data, but it should be interpreted with caution, especially when comparing models with different numbers of predictors (adjusted R² is often preferred then).</p>
            </div>
        </div>
    </div>
    <!-- 7-MAVZU TUGASHI -->

    <!-- 8-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">8. Interval estimates. Confidence level and confidence interval.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>In statistical inference, we often want to estimate an unknown population parameter (e.g., population mean μ, population proportion p) using sample data. While a point estimate provides a single best guess, an interval estimate provides a range of plausible values for the parameter.</p>

                <h5>Interval Estimate:</h5>
                <p>An <strong>interval estimate</strong> is a range of values, calculated from sample data, that is likely to contain the true value of an unknown population parameter. It acknowledges the uncertainty inherent in estimating from a sample.</p>
                <p>An interval estimate takes the form:</p>
                <pre class="formula">Point Estimate ± Margin of Error</pre>
                <p>The margin of error depends on the desired confidence level, the variability of the data (e.g., standard deviation), and the sample size.</p>

                <h5>Confidence Level:</h5>
                <p>The <strong>confidence level</strong> is the probability (often expressed as a percentage, e.g., 90%, 95%, 99%) that the method used to construct the interval will produce an interval that captures the true population parameter, *if the sampling process were repeated many times*. It reflects the reliability or success rate of the estimation procedure, not the probability that a *specific* calculated interval contains the true parameter.</p>
                <p>Common confidence levels are:</p>
                <ul>
                    <li>90% (α = 0.10)</li>
                    <li>95% (α = 0.05)</li>
                    <li>99% (α = 0.01)</li>
                </ul>
                <p>Here, α (alpha) is the significance level, representing the probability that the interval will *not* contain the true parameter (1 - confidence level).</p>

                <h5>Confidence Interval (CI):</h5>
                <p>A <strong>confidence interval</strong> is the actual computed range [lower bound, upper bound] obtained from a specific sample using a particular confidence level. Once a CI is calculated from a sample, it either contains the true parameter or it does not. The confidence level tells us about the long-run performance of the method.</p>
                <p><strong>Interpretation:</strong></p>
                <p>If we construct a 95% confidence interval for a population mean μ, say [L, U], we interpret it as: "We are 95% confident that this specific interval [L, U] contains the true population mean μ." This means that if we were to take many random samples of the same size from the population and construct a 95% CI for each sample, about 95% of those intervals would actually contain μ.</p>

                <h5>General Form for a Confidence Interval for a Mean (known σ or large n):</h5>
                <pre class="formula">x̄ ± z<sub>α/2</sub> * (σ / √n)</pre>
                <p>Where:</p>
                <ul>
                    <li>x̄ is the sample mean.</li>
                    <li>z<sub>α/2</sub> is the critical value from the standard normal distribution corresponding to the confidence level (e.g., for 95% CI, z<sub>α/2</sub> ≈ 1.96).</li>
                    <li>σ is the population standard deviation (if unknown and n is small, s is used and z is replaced by t from the t-distribution).</li>
                    <li>n is the sample size.</li>
                </ul>
                <p>The width of the confidence interval depends on:</p>
                <ul>
                    <li><strong>Confidence Level:</strong> Higher confidence level → wider interval.</li>
                    <li><strong>Sample Standard Deviation (s or σ):</strong> Higher variability → wider interval.</li>
                    <li><strong>Sample Size (n):</strong> Larger sample size → narrower interval.</li>
                </ul>
            </div>
        </div>
    </div>
    <!-- 8-MAVZU TUGASHI -->

    <!-- 9-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">9. Distribution as statistical estimates of non-normal parameters.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>This topic refers to the use of <strong>sampling distributions</strong> of estimators to make inferences (like constructing confidence intervals or performing hypothesis tests) about population parameters, especially when these parameters belong to non-normal distributions, or when the parameters themselves are not means (e.g., variances, proportions).</p>

                <h5>Key Concepts:</h5>
                <ul>
                    <li>
                        <strong>Estimators and Sampling Distributions:</strong>
                        <p>An estimator (e.g., sample mean x̄, sample variance s², sample proportion p̂) is a statistic calculated from a sample to estimate an unknown population parameter (e.g., population mean μ, population variance σ², population proportion p). Since estimators vary from sample to sample, they are random variables and have their own probability distributions, known as <strong>sampling distributions</strong>.</p>
                    </li>
                    <li>
                        <strong>Central Limit Theorem (CLT):</strong>
                        <p>Even if the underlying population distribution is not normal, the sampling distribution of the sample mean (x̄) tends to be approximately normal for sufficiently large sample sizes (usually n ≥ 30). This is crucial for making inferences about the population mean μ.</p>
                    </li>
                    <li>
                        <strong>Estimating Parameters of Non-Normal Distributions:</strong>
                        <p>Many real-world phenomena are not normally distributed (e.g., exponential, Poisson, binomial, gamma distributions). We still need to estimate their parameters.</p>
                        <ul>
                            <li><strong>Example (Binomial):</strong> If we want to estimate the success probability 'p' of a binomial distribution, we use the sample proportion p̂ = k/n (number of successes / number of trials). For large n, the sampling distribution of p̂ can be approximated by a normal distribution.</li>
                            <li><strong>Example (Poisson):</strong> The parameter λ (mean rate) of a Poisson distribution can be estimated by the sample mean x̄. For large λ, the Poisson distribution itself can be approximated by a normal distribution.</li>
                            <li><strong>Example (Exponential):</strong> The rate parameter λ of an exponential distribution is often estimated by 1/x̄. The sampling distribution of x̄ (and thus 1/x̄) can be complex but may involve distributions like the Gamma distribution.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Estimating Non-Mean Parameters (e.g., Variance):</strong>
                        <p>If the population is normal, the sampling distribution of the sample variance s² (when scaled) follows a <strong>Chi-squared (χ²) distribution</strong>. This allows us to make inferences about the population variance σ².</p>
                        <pre class="formula">(n-1)s² / σ² ~ χ²<sub>(n-1)</sub></pre>
                    </li>
                    <li>
                        <strong>Use of t-distribution:</strong>
                        <p>When estimating the mean μ of a normally distributed population with an unknown population standard deviation σ, and using the sample standard deviation s, the statistic (x̄ - μ) / (s/√n) follows a <strong>t-distribution</strong> with n-1 degrees of freedom. This is also robust to moderate departures from normality, especially for larger sample sizes.</p>
                    </li>
                    <li>
                        <strong>Bootstrap Methods:</strong>
                        <p>For complex distributions or when analytical sampling distributions are unknown, resampling methods like bootstrapping can be used to approximate the sampling distribution of an estimator and construct confidence intervals, even for non-normal parameters.</p>
                    </li>
                </ul>
                <p>In summary, statistical inference often relies on knowing or approximating the sampling distribution of an estimator. While normality assumptions simplify many procedures, various techniques and distributions (CLT, Chi-squared, t-distribution, approximations for proportions, bootstrapping) allow us to estimate and make inferences about parameters from non-normal distributions or parameters other than the mean.</p>
            </div>
        </div>
    </div>
    <!-- 9-MAVZU TUGASHI -->

    <!-- 10-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">10. Statistical, classical, and geometrical definitions of probability.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>Probability is a measure of the likelihood of an event occurring. There are several ways to define and interpret probability.</p>

                <h5>Classical Definition of Probability:</h5>
                <p>This definition applies when an experiment has a finite number of <strong>equally likely</strong> and <strong>mutually exclusive</strong> outcomes.</p>
                <p>If an experiment can result in N equally likely and mutually exclusive outcomes, and M of these outcomes are favorable to an event A, then the probability of event A is given by:</p>
                <pre class="formula">P(A) = M / N</pre>
                <p>Where:</p>
                <ul>
                    <li><strong>M</strong> is the number of outcomes favorable to event A.</li>
                    <li><strong>N</strong> is the total number of possible elementary outcomes.</li>
                </ul>
                <p><em>Example:</em> The probability of rolling a '3' on a fair six-sided die is P(rolling a 3) = 1/6, because there is 1 favorable outcome (rolling a 3) and 6 total equally likely outcomes {1, 2, 3, 4, 5, 6}.</p>
                <p><em>Limitation:</em> This definition is restrictive as it requires equally likely outcomes and a finite sample space.</p>

                <h5>Statistical (or Empirical/Frequency) Definition of Probability:</h5>
                <p>This definition is based on the relative frequency of an event in a large number of repeated trials conducted under identical conditions.</p>
                <p>If an experiment is repeated n times, and event A occurs n(A) times, then the relative frequency of event A is n(A)/n. The probability of event A is defined as the limit of this relative frequency as the number of trials n approaches infinity:</p>
                <pre class="formula">P(A) = lim<sub>n→∞</sub> [n(A) / n]</pre>
                <p><em>Example:</em> If a coin is flipped 1000 times and heads appears 503 times, the relative frequency of heads is 503/1000 = 0.503. The probability of heads is approached as the number of flips becomes very large.</p>
                <p><em>Advantage:</em> Applicable even when outcomes are not equally likely or the sample space is not easily enumerated. It's based on observation.</p>
                <p><em>Limitation:</em> It requires a large number of trials to get a good estimate, and we can never truly perform an infinite number of trials.</p>

                <h5>Geometrical Definition of Probability:</h5>
                <p>This definition applies when the sample space can be represented as a geometric region (e.g., length, area, volume) and outcomes are considered to be uniformly distributed over this region.</p>
                <p>If the set of all possible outcomes of an experiment can be represented by a region Ω in a geometric space (with measure m(Ω)), and the outcomes favorable to an event A form a sub-region A' within Ω (with measure m(A')), then the probability of event A is:</p>
                <pre class="formula">P(A) = measure(A') / measure(Ω)</pre>
                <p>Where "measure" can refer to length, area, or volume depending on the dimension of the space.</p>
                <p><em>Example:</em> If a point is chosen at random from a square of side length 'L', the probability that it falls within a smaller circle of radius 'r' inscribed within the square is P(point in circle) = (Area of circle) / (Area of square) = (πr²) / L².</p>
                <p><em>Limitation:</em> Assumes a uniform distribution of outcomes over the geometric space.</p>

                <p><em>Note:</em> The axiomatic definition of probability by Kolmogorov provides a more general and mathematically rigorous foundation, encompassing these classical, statistical, and geometrical interpretations as special cases.</p>
            </div>
        </div>
    </div>
    <!-- 10-MAVZU TUGASHI -->

    <!-- 11-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">11. Interval estimates. Confidence level and confidence interval.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>In statistical inference, we often want to estimate an unknown population parameter (e.g., population mean μ, population proportion p) using sample data. While a point estimate provides a single best guess, an interval estimate provides a range of plausible values for the parameter.</p>

                <h5>Interval Estimate:</h5>
                <p>An <strong>interval estimate</strong> is a range of values, calculated from sample data, that is likely to contain the true value of an unknown population parameter. It acknowledges the uncertainty inherent in estimating from a sample.</p>
                <p>An interval estimate takes the form:</p>
                <pre class="formula">Point Estimate ± Margin of Error</pre>
                <p>The margin of error depends on the desired confidence level, the variability of the data (e.g., standard deviation), and the sample size.</p>

                <h5>Confidence Level:</h5>
                <p>The <strong>confidence level</strong> is the probability (often expressed as a percentage, e.g., 90%, 95%, 99%) that the method used to construct the interval will produce an interval that captures the true population parameter, *if the sampling process were repeated many times*. It reflects the reliability or success rate of the estimation procedure, not the probability that a *specific* calculated interval contains the true parameter.</p>
                <p>Common confidence levels are:</p>
                <ul>
                    <li>90% (α = 0.10)</li>
                    <li>95% (α = 0.05)</li>
                    <li>99% (α = 0.01)</li>
                </ul>
                <p>Here, α (alpha) is the significance level, representing the probability that the interval will *not* contain the true parameter (1 - confidence level).</p>

                <h5>Confidence Interval (CI):</h5>
                <p>A <strong>confidence interval</strong> is the actual computed range [lower bound, upper bound] obtained from a specific sample using a particular confidence level. Once a CI is calculated from a sample, it either contains the true parameter or it does not. The confidence level tells us about the long-run performance of the method.</p>
                <p><strong>Interpretation:</strong></p>
                <p>If we construct a 95% confidence interval for a population mean μ, say [L, U], we interpret it as: "We are 95% confident that this specific interval [L, U] contains the true population mean μ." This means that if we were to take many random samples of the same size from the population and construct a 95% CI for each sample, about 95% of those intervals would actually contain μ.</p>

                <h5>General Form for a Confidence Interval for a Mean (known σ or large n):</h5>
                <pre class="formula">x̄ ± z<sub>α/2</sub> * (σ / √n)</pre>
                <p>Where:</p>
                <ul>
                    <li>x̄ is the sample mean.</li>
                    <li>z<sub>α/2</sub> is the critical value from the standard normal distribution corresponding to the confidence level (e.g., for 95% CI, z<sub>α/2</sub> ≈ 1.96).</li>
                    <li>σ is the population standard deviation (if unknown and n is small, s is used and z is replaced by t from the t-distribution).</li>
                    <li>n is the sample size.</li>
                </ul>
                <p>The width of the confidence interval depends on:</p>
                <ul>
                    <li><strong>Confidence Level:</strong> Higher confidence level → wider interval.</li>
                    <li><strong>Sample Standard Deviation (s or σ):</strong> Higher variability → wider interval.</li>
                    <li><strong>Sample Size (n):</strong> Larger sample size → narrower interval.</li>
                </ul>
            </div>
        </div>
    </div>
    <!-- 11-MAVZU TUGASHI -->

    <!-- 12-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">12. Formula of total probability. Bayes' formula.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>The formula of total probability and Bayes' formula (or Bayes' Theorem) are fundamental concepts in probability theory that deal with conditional probabilities and updating beliefs based on new evidence.</p>

                <h5>Formula of Total Probability:</h5>
                <p>The formula of total probability allows us to find the probability of an event A by considering all possible mutually exclusive scenarios (a partition of the sample space) under which A can occur.</p>
                <p>Let B₁, B₂, ..., Bₙ be a set of events that form a <strong>partition</strong> of the sample space Ω. This means:</p>
                <ol>
                    <li>The events Bᵢ are mutually exclusive (Bᵢ ∩ Bⱼ = Ø for i ≠ j).</li>
                    <li>The union of the events is the entire sample space (B₁ ∪ B₂ ∪ ... ∪ Bₙ = Ω).</li>
                    <li>P(Bᵢ) > 0 for all i.</li>
                </ol>
                <p>Then, for any event A in the sample space Ω, its probability can be calculated as:</p>
                <pre class="formula">P(A) = Σ<sub>i=1</sub><sup>n</sup> P(A|Bᵢ)P(Bᵢ)</pre>
                <p>Or expanded:</p>
                <pre class="formula">P(A) = P(A|B₁)P(B₁) + P(A|B₂)P(B₂) + ... + P(A|Bₙ)P(Bₙ)</pre>
                <p>Where P(A|Bᵢ) is the conditional probability of event A occurring given that event Bᵢ has occurred.</p>

                <h5>Bayes' Formula (or Bayes' Theorem):</h5>
                <p>Bayes' formula describes how to update the probability of a hypothesis (an event Bᵢ) based on new evidence (the occurrence of an event A). It relates the conditional probability P(Bᵢ|A) to P(A|Bᵢ).</p>
                <p>Using the definition of conditional probability, P(Bᵢ|A) = P(A ∩ Bᵢ) / P(A) and P(A|Bᵢ) = P(A ∩ Bᵢ) / P(Bᵢ). Thus, P(A ∩ Bᵢ) = P(A|Bᵢ)P(Bᵢ).</p>
                <p>Substituting this into the first equation for P(Bᵢ|A), we get Bayes' Formula:</p>
                <pre class="formula">P(Bᵢ|A) = [P(A|Bᵢ)P(Bᵢ)] / P(A)</pre>
                <p>Using the formula of total probability for P(A) in the denominator, Bayes' Formula can also be written as:</p>
                <pre class="formula">P(Bᵢ|A) = [P(A|Bᵢ)P(Bᵢ)] / [Σ<sub>j=1</sub><sup>n</sup> P(A|Bⱼ)P(Bⱼ)]</pre>
                <p>In this context:</p>
                <ul>
                    <li><strong>P(Bᵢ)</strong> is the <strong>prior probability</strong> of event Bᵢ (our belief before observing A).</li>
                    <li><strong>P(A|Bᵢ)</strong> is the <strong>likelihood</strong> of observing event A given that Bᵢ is true.</li>
                    <li><strong>P(Bᵢ|A)</strong> is the <strong>posterior probability</strong> of event Bᵢ (our updated belief after observing A).</li>
                    <li><strong>P(A)</strong> is the marginal likelihood or evidence.</li>
                </ul>
                <p>Bayes' theorem is widely used in fields like medical diagnosis, spam filtering, and machine learning.</p>
            </div>
        </div>
    </div>
    <!-- 12-MAVZU TUGASHI -->

    <!-- 13-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">13. Sequence of independent trials. Bernoulli's formula. Poisson's theorem.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <p>These concepts deal with scenarios involving multiple repetitions of an experiment where each repetition is independent of the others.</p>

                <h5>Sequence of Independent Trials:</h5>
                <p>A sequence of trials is said to be <strong>independent</strong> if the outcome of each trial does not influence or depend on the outcome of any other trial in the sequence. If the probability of a particular event remains the same for each trial, these are often called <strong>Bernoulli trials</strong> (if there are only two outcomes).</p>

                <h5>Bernoulli's Formula (Binomial Probability Formula):</h5>
                <p>Bernoulli's formula calculates the probability of obtaining exactly <em>k</em> successes in a sequence of <em>n</em> independent Bernoulli trials. A Bernoulli trial is an experiment with only two possible outcomes, typically labeled "success" (S) and "failure" (F).</p>
                <p>Let:</p>
                <ul>
                    <li><strong>n</strong> = total number of independent trials.</li>
                    <li><strong>k</strong> = exact number of successes desired.</li>
                    <li><strong>p</strong> = probability of success on a single trial (constant for each trial).</li>
                    <li><strong>q</strong> = 1 - p = probability of failure on a single trial.</li>
                </ul>
                <p>The probability of exactly k successes in n trials, Pₙ(k) or P(X=k), is given by Bernoulli's formula (also known as the binomial probability formula):</p>
                <pre class="formula">Pₙ(k) = C(n,k) * p<sup>k</sup> * q<sup>(n-k)</sup></pre>
                <p>Where C(n,k) is the binomial coefficient, "n choose k", calculated as:</p>
                <pre class="formula">C(n,k) = n! / [k! * (n-k)!]</pre>
                <p>The random variable X representing the number of successes in n Bernoulli trials follows a Binomial Distribution, denoted X ~ B(n,p).</p>

                <h5>Poisson's Theorem (Law of Rare Events / Poisson Approximation to Binomial):</h5>
                <p>Poisson's theorem states that the binomial distribution B(n,p) can be approximated by the Poisson distribution Po(λ) with parameter λ = np, under certain conditions. This approximation is generally good when:</p>
                <ul>
                    <li><strong>n</strong> (number of trials) is very large (e.g., n ≥ 100).</li>
                    <li><strong>p</strong> (probability of success) is very small (e.g., p ≤ 0.05 or p ≤ 0.1).</li>
                    <li>The product <strong>λ = np</strong> is a moderate constant (e.g., np ≤ 10 or np ≤ 20, though guidelines vary).</li>
                </ul>
                <p>If these conditions hold, the probability of k successes can be approximated using the Poisson probability mass function:</p>
                <pre class="formula">P(X=k) ≈ (e<sup>-λ</sup> * λ<sup>k</sup>) / k!</pre>
                <p>Where:</p>
                <ul>
                    <li><strong>λ = np</strong> is the average number of successes in n trials.</li>
                    <li><strong>e</strong> is the base of the natural logarithm (approximately 2.71828).</li>
                </ul>
                <p>The Poisson distribution is often used to model the number of occurrences of a "rare" event over a large number of opportunities or within a specific interval of time or space (e.g., number of typos per page, number of calls to a call center per hour).</p>
            </div>
        </div>
    </div>
    <!-- 13-MAVZU TUGASHI -->

    <!-- 14-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">14. Concept of a continuous random variable. Distribution function and its properties.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <h5>Concept of a Continuous Random Variable:</h5>
                <p>A <strong>continuous random variable</strong> is a random variable that can take on any value within a specified range or interval (or multiple intervals) on the real number line. Unlike discrete random variables, which have countable outcomes, continuous random variables have an uncountably infinite number of possible values.</p>
                <p>Key characteristics:</p>
                <ul>
                    <li>Its possible values form a continuous spectrum.</li>
                    <li>The probability that a continuous random variable X takes on any single specific value is zero, i.e., P(X=c) = 0 for any constant c.</li>
                    <li>Probabilities are defined for intervals, e.g., P(a ≤ X ≤ b) or P(X > c).</li>
                    <li>For continuous random variables, P(a ≤ X ≤ b) = P(a < X ≤ b) = P(a ≤ X < b) = P(a < X < b) because the probability at single points is zero.</li>
                </ul>
                <p><em>Examples:</em> Height of a person, temperature, time taken to complete a task, length of an object.</p>

                <h5>Distribution Function (Cumulative Distribution Function - CDF):</h5>
                <p>The <strong>distribution function</strong>, more commonly known as the <strong>Cumulative Distribution Function (CDF)</strong>, denoted as F(x), is defined for any random variable X (both discrete and continuous) as the probability that X takes on a value less than or equal to x.</p>
                <pre class="formula">F(x) = P(X ≤ x)</pre>

                <h5>Properties of the CDF for a Continuous Random Variable:</h5>
                <ol>
                    <li>
                        <strong>Range of F(x):</strong>
                        <pre class="formula">0 ≤ F(x) ≤ 1</pre>
                        <p>Since F(x) is a probability, it must be between 0 and 1.</p>
                    </li>
                    <li>
                        <strong>Non-decreasing Function:</strong>
                        <p>If a < b, then F(a) ≤ F(b). This means the CDF never decreases as x increases.</p>
                    </li>
                    <li>
                        <strong>Limits:</strong>
                        <pre class="formula">lim<sub>x→-∞</sub> F(x) = 0</pre>
                        <pre class="formula">lim<sub>x→+∞</sub> F(x) = 1</pre>
                        <p>As x approaches negative infinity, the probability P(X ≤ x) approaches 0. As x approaches positive infinity, the probability P(X ≤ x) approaches 1.</p>
                    </li>
                    <li>
                        <strong>Continuity:</strong>
                        <p>For a continuous random variable, its CDF F(x) is continuous everywhere. There are no "jumps" in the graph of F(x).</p>
                    </li>
                    <li>
                        <strong>Calculating Probabilities for Intervals:</strong>
                        <pre class="formula">P(a < X ≤ b) = F(b) - F(a)</pre>
                        <p>Since P(X=a) = 0 for continuous RVs, this also means P(a ≤ X ≤ b) = F(b) - F(a).</p>
                    </li>
                    <li>
                        <strong>Relationship with Probability Density Function (PDF):</strong>
                        <p>If f(t) is the Probability Density Function (PDF) of a continuous random variable X, then the CDF is given by:</p>
                        <pre class="formula">F(x) = P(X ≤ x) = ∫<sub>-∞</sub><sup>x</sup> f(t) dt</pre>
                        <p>Conversely, the PDF can be obtained by differentiating the CDF (where F is differentiable):</p>
                        <pre class="formula">f(x) = dF(x) / dx = F'(x)</pre>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <!-- 14-MAVZU TUGASHI -->

    <!-- 15-MAVZU BOSHLANISHI -->
    <div class="accordion-item">
        <button class="accordion-button" aria-expanded="false">
            <span class="accordion-title">15. Two-dimensional random variable. Discrete two-dimensional random variable and its distribution law.</span>
            <span class="accordion-icon open-icon">+</span>
            <span class="accordion-icon close-icon">×</span>
        </button>
        <div class="accordion-content">
            <div class="accordion-content-inner">
                <h5>Two-dimensional Random Variable (Random Vector):</h5>
                <p>A <strong>two-dimensional random variable</strong>, also known as a <strong>bivariate random variable</strong> or a <strong>random vector</strong> of dimension 2, is an ordered pair of random variables, say (X, Y), defined on the same sample space Ω. It assigns a pair of real numbers (x, y) to each outcome ω in the sample space Ω.</p>
                <p><em>Example:</em> If we randomly select a student, X could be their height and Y could be their weight. (X,Y) would be a two-dimensional random variable.</p>
                <p>The behavior of a two-dimensional random variable is described by its <strong>joint distribution function</strong> (or joint CDF):</p>
                <pre class="formula">F(x,y) = P(X ≤ x, Y ≤ y)</pre>
                <p>This gives the probability that X is less than or equal to x AND Y is less than or equal to y.</p>

                <h5>Discrete Two-dimensional Random Variable:</h5>
                <p>A two-dimensional random variable (X,Y) is <strong>discrete</strong> if both X and Y are discrete random variables. This means that the pair (X,Y) can take on a finite or countably infinite set of distinct pairs of values (xᵢ, yⱼ).</p>

                <h5>Distribution Law (Joint Probability Mass Function - PMF):</h5>
                <p>For a discrete two-dimensional random variable (X,Y), its distribution law is described by the <strong>joint probability mass function (joint PMF)</strong>, denoted as p(xᵢ, yⱼ) or P(X=xᵢ, Y=yⱼ).</p>
                <pre class="formula">p(xᵢ, yⱼ) = P(X=xᵢ, Y=yⱼ)</pre>
                <p>This function gives the probability that the random variable X takes the value xᵢ *and simultaneously* the random variable Y takes the value yⱼ.</p>

                <h6>Properties of the Joint PMF:</h6>
                <ol>
                    <li>
                        <strong>Non-negativity:</strong>
                        <pre class="formula">p(xᵢ, yⱼ) ≥ 0</pre>
                        <p>The probability of any specific pair of outcomes must be non-negative.</p>
                    </li>
                    <li>
                        <strong>Normalization:</strong>
                        <pre class="formula">Σ<sub>i</sub> Σ<sub>j</sub> p(xᵢ, yⱼ) = 1</pre>
                        <p>The sum of the probabilities over all possible pairs of (xᵢ, yⱼ) must equal 1.</p>
                    </li>
                </ol>

                <h6>Representation:</h6>
                <p>The joint PMF of a discrete two-dimensional random variable is often represented in a <strong>joint probability distribution table</strong> (also called a contingency table for frequencies). The rows might represent the possible values of X, and the columns the possible values of Y (or vice versa). Each cell (i, j) in the table contains the probability p(xᵢ, yⱼ).</p>
                <p><em>Example Table:</em></p>
                <table border="1" style="border-collapse: collapse; text-align: center;">
                    <tr>
                        <td rowspan="2">Y \ X</td>
                        <td colspan="2">Values of X</td>
                        <td rowspan="2">P(Y=y<sub>j</sub>) (Marginal for Y)</td>
                    </tr>
                    <tr>
                        <td>x₁</td>
                        <td>x₂</td>
                    </tr>
                    <tr>
                        <td>y₁</td>
                        <td>p(x₁,y₁)</td>
                        <td>p(x₂,y₁)</td>
                        <td>P(Y=y₁)</td>
                    </tr>
                    <tr>
                        <td>y₂</td>
                        <td>p(x₁,y₂)</td>
                        <td>p(x₂,y₂)</td>
                        <td>P(Y=y₂)</td>
                    </tr>
                    <tr>
                        <td>P(X=x<sub>i</sub>) (Marginal for X)</td>
                        <td>P(X=x₁)</td>
                        <td>P(X=x₂)</td>
                        <td>1</td>
                    </tr>
                </table>

                <h6>Marginal Distributions:</h6>
                <p>From the joint PMF, we can obtain the individual (marginal) PMFs for X and Y:</p>
                <pre class="formula">P(X=xᵢ) = Σ<sub>j</sub> p(xᵢ, yⱼ)   (Sum over all possible y values for a fixed xᵢ)</pre>
                <pre class="formula">P(Y=yⱼ) = Σ<sub>i</sub> p(xᵢ, yⱼ)   (Sum over all possible x values for a fixed yⱼ)</pre>
                <p>These are the row sums and column sums in the joint probability table, respectively.</p>
            </div>
        </div>
    </div>
    <!-- 15-MAVZU TUGASHI -->



</body>
</html>

<script src="script.js"></script>
</body>
</html>